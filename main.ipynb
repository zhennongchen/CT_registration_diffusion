{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91803cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/host/d/Github/')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import CT_registration_diffusion.functions_collection as ff\n",
    "import CT_registration_diffusion.Build_lists.Build_list as Build_list\n",
    "import CT_registration_diffusion.Data_processing as Data_processing\n",
    "import CT_registration_diffusion.Generator as Generator\n",
    "import CT_registration_diffusion.model.model as model\n",
    "import CT_registration_diffusion.model.train_engine as train_engine\n",
    "import CT_registration_diffusion.model.predict_engine as predict_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a6ac1",
   "metadata": {},
   "source": [
    "### define our trial name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca09346",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'trial_1_movingTF0_MSE_weight0.01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344f30e",
   "metadata": {},
   "source": [
    "### step 0: define some pre-set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "image_size = [224,224,96]\n",
    "cutoff_range = [-200,250]\n",
    "\n",
    "# train\n",
    "train_batch_size = 1 # training batch size,  如果GPU内存不够，可以把这个值设成1\n",
    "accumulation_steps = 5 # gradient accumulation steps to simulate larger batch size， 如果GPU内存不够，可以把这个值设成更小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d9eb5",
   "metadata": {},
   "source": [
    "### step 1: define patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c788b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image folder: /host/d/Data/4DCT/DIR_LAB/Case1/cropped_image\n"
     ]
    }
   ],
   "source": [
    "# change the excel path to your own path\n",
    "patient_list_spreadsheet = os.path.join('/host/d/Data/4DCT/Patient_lists/ct_list.xlsx')\n",
    "build_sheet =  Build_list.Build(patient_list_spreadsheet)\n",
    "\n",
    "# define train\n",
    "batch_list_train, dataset_id_list_train, case_id_list_train, image_folder_list_train = build_sheet.__build__(batch_list = [0])\n",
    "# 先用一个case跑通代码\n",
    "batch_list_train = batch_list_train[0:1]\n",
    "dataset_id_list_train = dataset_id_list_train[0:1]\n",
    "case_id_list_train = case_id_list_train[0:1]\n",
    "image_folder_list_train = image_folder_list_train[0:1]\n",
    "\n",
    "\n",
    "# define validation\n",
    "batch_list_val, dataset_id_list_val, case_id_list_val, image_folder_list_val = build_sheet.__build__(batch_list = [3])\n",
    "# 先用一个case跑通代码, train 和 val暂时先用同一个case\n",
    "batch_list_val = batch_list_train\n",
    "dataset_id_list_val = dataset_id_list_train\n",
    "case_id_list_val = case_id_list_train\n",
    "image_folder_list_val = image_folder_list_train\n",
    "\n",
    "# print一个路径来看看\n",
    "print('train image folder:', image_folder_list_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661247",
   "metadata": {},
   "source": [
    "### step 2: define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ffdca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training generator\n",
    "only_use_tf0_as_moving = True # if set True, only use time frame 0 as moving image, otherwise randomly select moving time frame\n",
    "generator_train = Generator.Dataset_4DCT(\n",
    "    image_folder_list = image_folder_list_train,\n",
    "    \n",
    "    image_size = image_size, # target image size after center-crop\n",
    "    cutoff_range = cutoff_range, # default cutoff range for CT images\n",
    "\n",
    "    num_of_pairs_each_case = 10, # 在一个4DCT case中，随机选取多少对time frames（比如说我们选了time frame 0和time frame 2作为一对，time frame 1和time frame 3作为另一对，那么num_of_pairs_each_case就是2）\n",
    "    preset_paired_tf = None, # 预设每个case中time frame的配对情况，比如说[(0,2),(1,3)]表示time frame 0和2作为一对，1和3作为一对。如果设置了这个参数，那么num_of_pairs_each_case就需要和这个list的长度一致。如果是None，那么每次从4DCT中随机选取num_of_pairs_each_case对time frames。\n",
    "    only_use_tf0_as_moving = only_use_tf0_as_moving, \n",
    "\n",
    "    shuffle = True,\n",
    "\n",
    "    augment = True, # whether to do data augmentation, in training set it to True\n",
    "    augment_frequency = 0.5, )\n",
    "\n",
    "# define validation generator\n",
    "# 和training不同的是，我们在validation中要尽量保持数据的一致性，因此不进行shuffle和data augmentation。同时我们要设定preset_paired_tf，确保每次选取的time frame配对是一样的。\n",
    "preset_paired_tf_val = [(0,1),(0,2),(0,4), (0,5),(0,6),(0,8)] # 预设validation中每个case的time frame配对情况\n",
    "generator_val = Generator.Dataset_4DCT(\n",
    "    image_folder_list = image_folder_list_val,\n",
    "    image_size = image_size, \n",
    "    cutoff_range = cutoff_range, \n",
    "\n",
    "    num_of_pairs_each_case = len(preset_paired_tf_val), \n",
    "    preset_paired_tf = preset_paired_tf_val, \n",
    "    only_use_tf0_as_moving = only_use_tf0_as_moving,\n",
    "    shuffle = False,\n",
    "    augment = False, # whether to do data augmentation\n",
    "    augment_frequency = 0.0, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2932491",
   "metadata": {},
   "source": [
    "### step 3: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5013702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in out is :  [(4, 8), (8, 16), (16, 32), (32, 64)]\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "our_model = model.Unet(\n",
    "    problem_dimension = '3D',  # we are solving a 3D image registration problem\n",
    "  \n",
    "    input_channels = 2, # =1 如果只有一个4DCT time frame(比如只有time frame 0）作为模型输入；=2 如果有两个4DCT time frames（比如time frame 0和time frame 2作为moving和fixed image）作为模型输入\n",
    "    out_channels = 3,  # =2 for 2D deformation field; =3 for 3D deformation field\n",
    "\n",
    "    initial_dim = 4,  # default initial feature dimension after first conv layer\n",
    "    dim_mults = (2,4,8,16),\n",
    "    groups = 4,\n",
    "      \n",
    "    full_attn_paths = (None, None, None, None), # these are for downsampling and upsampling paths， 现在都是None因为要考虑GPU内存\n",
    "    full_attn_bottleneck = False, # this is for the middle bottleneck layer， 现在是None因为要考虑GPU内存\n",
    "    act = 'ReLU',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39224e6",
   "metadata": {},
   "source": [
    "### step 4: build trainer and start to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a1aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_weight = 0.01 # weight for deformation field smoothness regularization term， 这个是要通过测试来确定最佳取值\n",
    "\n",
    "total_epochs = 4000\n",
    "save_models_every = 20 # save model every N epochs，训练样本越少这个数字越大，样本越大这个数字可以越小（通常我设成1-5之间，如果只有一个case我会设成20-50）\n",
    "validation_every = 10000#save_models_every # validate every N epochs, should be same as save_models_every\n",
    "# where to save your model weights? Change this path to your own path\n",
    "results_folder = os.path.join('/host/d/projects/registration/models', trial_name, 'models')\n",
    "ff.make_folder([os.path.basename(results_folder), results_folder])\n",
    "\n",
    "trainer = train_engine.Trainer(\n",
    "        our_model,\n",
    "        generator_train,\n",
    "        generator_val,\n",
    "        train_batch_size = train_batch_size,\n",
    "        accum_iter= accumulation_steps,\n",
    "\n",
    "        regularization_weight = regularization_weight,\n",
    "        train_num_steps = total_epochs,\n",
    "        results_folder = results_folder,\n",
    "       \n",
    "        train_lr_decay_every = 200, \n",
    "        save_models_every = save_models_every,\n",
    "        validation_every = validation_every,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d14c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "### do we have pre-trained model?\n",
    "pretrained_model = None\n",
    "\n",
    "# what is the start epoch?\n",
    "start_epoch = 0 # if no pre-trained model, start from epoch 0, else start from the loaded epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  1\n",
      "learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average loss: 0.0107, average similarity loss: 0.0106, average regularization loss: 0.0122:   0%|          | 1/4000 [00:14<15:47:20, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  2\n",
      "learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average loss: 0.0082, average similarity loss: 0.0081, average regularization loss: 0.0095:   0%|          | 2/4000 [00:38<22:02:41, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  3\n",
      "learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average loss: 0.0063, average similarity loss: 0.0062, average regularization loss: 0.0090:   0%|          | 3/4000 [01:08<27:15:23, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  4\n",
      "learning rate:  0.0001\n"
     ]
    }
   ],
   "source": [
    "# # start training\n",
    "trainer.train(pre_trained_model = pretrained_model, start_step = start_epoch)\n",
    "# #如果跑不动（GPU内存不足），\n",
    "# 1.可以尝试减小model里initial_dim的值，比如改成4或者2\n",
    "# 2.可以尝试减小train_batch_size\n",
    "# 3.可以尝试减小accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842e683",
   "metadata": {},
   "source": [
    "### step 5: build predictor and use trained model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c126b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define save folder\n",
    "save_folder = os.path.join('/host/d/projects/registration/models', trial_name, 'results')\n",
    "ff.make_folder([os.path.basename(save_folder), save_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc829b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the excel path to your own path\n",
    "patient_list_spreadsheet = os.path.join('/host/d/Data/4DCT/Patient_lists/ct_list.xlsx')\n",
    "build_sheet =  Build_list.Build(patient_list_spreadsheet)\n",
    "\n",
    "# define test (作为展示我们先用train的case来跑一下)\n",
    "batch_list_tst, dataset_id_list_tst, case_id_list_tst, image_folder_list_tst = build_sheet.__build__(batch_list = [0])\n",
    "# 先用一个case跑通代码\n",
    "batch_list_tst = batch_list_tst[0:1]\n",
    "dataset_id_list_tst = dataset_id_list_tst[0:1]\n",
    "case_id_list_tst = case_id_list_tst[0:1]\n",
    "image_folder_list_tst = image_folder_list_tst[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e46fee",
   "metadata": {},
   "source": [
    "### step 5.2 define the pre-trained model (the epoch that has the best validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab68f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 60\n",
    "trained_model_file = os.path.join('/host/d/projects/registration/models', trial_name, 'models', 'model-' + str(epoch) + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2106bf",
   "metadata": {},
   "source": [
    "### step 5.3 do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8886a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/host/d/Github/CT_registration_diffusion/model/predict_engine.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(trained_model_filename, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case id: Case1 has 10 time frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted MVF_numpy shape: (3, 224, 224, 96)\n",
      "warped moving image shape: (224, 224, 96)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,case_id_list_tst.shape[0]):\n",
    "    \n",
    "    # find out how many time frames in this 4DCT\n",
    "    image_folder = image_folder_list_tst[i]\n",
    "    timeframes = ff.sort_timeframe(ff.find_all_target_files(['img*'], image_folder),2,'_')\n",
    "    print('case id:', case_id_list_tst[i], 'has', len(timeframes), 'time frames.')\n",
    "\n",
    "    # save folder for this case\n",
    "    save_folder_case = os.path.join(save_folder, case_id_list_tst[i], 'epoch_' + str(epoch))\n",
    "    ff.make_folder([os.path.basename(save_folder_case), save_folder_case])\n",
    "    \n",
    "    ### get the deformation fields for each time frame using the first time frame as moving image\n",
    "    for tf in range(5,6):# len(timeframes)):\n",
    "        moving_tf = 0\n",
    "        fixed_tf = tf\n",
    "\n",
    "        # define prediction generator\n",
    "        generator_pred = Generator.Dataset_4DCT(\n",
    "            image_folder_list = [image_folder_list_tst[i]],\n",
    "            image_size = image_size, \n",
    "            cutoff_range = cutoff_range, \n",
    "            only_use_tf0_as_moving=True,\n",
    "\n",
    "            num_of_pairs_each_case = 1, \n",
    "            preset_paired_tf = [(moving_tf, fixed_tf)], \n",
    "        )\n",
    "\n",
    "        # define predictor\n",
    "        predictor = predict_engine.Predictor(\n",
    "            our_model,\n",
    "            generator_pred,\n",
    "            batch_size = 1,\n",
    "        )\n",
    "\n",
    "        \n",
    "        # predict MVF\n",
    "        pred_MVF, pred_MVF_numpy, warped_moving_image_numpy = predictor.predict_MVF_and_apply(trained_model_filename = trained_model_file)\n",
    "        print('predicted MVF_numpy shape:', pred_MVF_numpy.shape)\n",
    "        print('warped moving image shape:', warped_moving_image_numpy.shape)\n",
    "\n",
    "        # save truth\n",
    "        moving_image_file = timeframes[moving_tf]\n",
    "        fixed_image_file = timeframes[fixed_tf]\n",
    "        moving_image_nii = nb.load(moving_image_file)\n",
    "        fixed_image_nii = nb.load(fixed_image_file)\n",
    "        affine = moving_image_nii.affine\n",
    "\n",
    "        nb.save(nb.Nifti1Image(moving_image_nii.get_fdata(), affine), os.path.join(save_folder_case, 'gt_tf' + str(moving_tf) + '.nii.gz'))\n",
    "        nb.save(nb.Nifti1Image(fixed_image_nii.get_fdata(), affine), os.path.join(save_folder_case, 'gt_tf' + str(fixed_tf) + '.nii.gz'))\n",
    "        # save warped moving image\n",
    "        nb.save(nb.Nifti1Image(warped_moving_image_numpy, affine), os.path.join(save_folder_case, 'warped_tf' + str(fixed_tf) + '.nii.gz'))\n",
    "        # save predicted MVF\n",
    "        nb.save(nb.Nifti1Image(pred_MVF_numpy[0,...], affine), os.path.join(save_folder_case, 'pred_MVF_tf' + str(fixed_tf) + '_x.nii.gz'))\n",
    "        nb.save(nb.Nifti1Image(pred_MVF_numpy[1,...], affine), os.path.join(save_folder_case, 'pred_MVF_tf' + str(fixed_tf) + '_y.nii.gz'))\n",
    "        nb.save(nb.Nifti1Image(pred_MVF_numpy[2,...], affine), os.path.join(save_folder_case, 'pred_MVF_tf' + str(fixed_tf) + '_z.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9004969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data range: -1.5182745 2.666669\n"
     ]
    }
   ],
   "source": [
    "print('data range:', np.min(pred_MVF_numpy), np.max(pred_MVF_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b686f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCC value between identical images: -25475170.0\n"
     ]
    }
   ],
   "source": [
    "import CT_registration_diffusion.model.loss as my_loss\n",
    "\n",
    "img = fixed_image_nii.get_fdata()[np.newaxis, np.newaxis, ...]\n",
    "img_torch = torch.from_numpy(img).float().to('cuda')\n",
    "similarity_metric = my_loss.NCCLoss()\n",
    "warped_torch = torch.from_numpy(warped_moving_image_numpy[np.newaxis, np.newaxis, ...]).float().to('cuda')\n",
    "value = similarity_metric(warped_torch, img_torch)\n",
    "print('NCC value between identical images:', value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81590c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
